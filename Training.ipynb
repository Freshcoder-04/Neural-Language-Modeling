{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10668787,"sourceType":"datasetVersion","datasetId":6607670}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nfrom nltk.tokenize import sent_tokenize, TreebankWordTokenizer\nimport pickle\n\nABBREVIATIONS = {\"Mr.\", \"Mrs.\", \"Dr.\", \"Ms.\", \"Prof.\", \"Sr.\", \"Jr.\", \"vs.\", \"etc.\"}\n\ndef preprocess_text(text):\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"URL\", text)\n    text = re.sub(r\"@\\w+\", \"MENTION\", text)\n    text = re.sub(r\"#\\w+\", \"HASHTAG\", text)\n    text = re.sub(r\"\\b\\d+(\\.\\d+)?\\s?%\", \"PERCENTAGE\", text)\n    text = re.sub(r\"\\b\\d+\\s?(years old|yrs old|yo|years|yrs)\\b\", \"AGE\", text, flags=re.IGNORECASE)\n    text = re.sub(r\"\\b\\d{1,2}:\\d{2}(?:\\s?[APap][Mm])?\\b\", \"TIME\", text)\n    text = re.sub(r\"\\b\\d+\\s?(seconds|minutes|hours|days|weeks|months|years)\\b\", \n                  \"TIME_PERIOD\", text, flags=re.IGNORECASE)\n    return text\n\ndef clean_text(text):\n    paragraphs = text.split(\"\\n\\n\")\n    cleaned_paragraphs = []\n\n    for paragraph in paragraphs:\n        cleaned_paragraph = re.sub(r\"\\s*\\n\\s*\", \" \", paragraph)\n        cleaned_paragraphs.append(cleaned_paragraph.strip())\n\n    return \"\\n\\n\".join(cleaned_paragraphs)\n\ndef fix_abbreviation_splits(text):\n    for abbr in ABBREVIATIONS:\n        text = re.sub(rf\"\\b{re.escape(abbr)}\\s\", abbr.replace(\".\", \"__TEMP__\") + \" \", text)\n    \n    return text\n\ndef restore_abbreviations(text):\n    return text.replace(\"__TEMP__\", \".\")\n\ndef custom_nlp_tokenizer(text):\n    text = preprocess_text(text)\n    text = clean_text(text)\n\n    tokenizer = TreebankWordTokenizer()\n\n    text = fix_abbreviation_splits(text)\n\n    sentences = sent_tokenize(text)\n\n    sentences = [restore_abbreviations(sent) for sent in sentences]\n\n    tokenized_sentences = []\n    for sentence in sentences:\n        tokens = tokenizer.tokenize(sentence)\n        tokens = ['<s>'] + tokens + ['</s>']\n        tokenized_sentences.append(tokens)\n\n    return tokenized_sentences\n\nif __name__ == '__main__':\n    text = '''\nI am Shravan. What are\nyou doing here?\nAre you\ngood, Mr. Bingley!!'''\n    result = custom_nlp_tokenizer(text)\n    print(\"Tokenized text:\", result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T15:27:04.140637Z","iopub.execute_input":"2025-02-08T15:27:04.141001Z","iopub.status.idle":"2025-02-08T15:27:05.258574Z","shell.execute_reply.started":"2025-02-08T15:27:04.140974Z","shell.execute_reply":"2025-02-08T15:27:05.257867Z"}},"outputs":[{"name":"stdout","text":"Tokenized text: [['<s>', 'I', 'am', 'Shravan', '.', '</s>'], ['<s>', 'What', 'are', 'you', 'doing', 'here', '?', '</s>'], ['<s>', 'Are', 'you', 'good', ',', 'Mr.', 'Bingley', '!', '</s>'], ['<s>', '!', '</s>']]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nfrom torch.utils.data import DataLoader, Dataset\n# from tokenizer import custom_nlp_tokenizer\nimport sys\nimport os\n\nclass FFNNLanguageModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_gram):\n        super(FFNNLanguageModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.fc1 = nn.Linear(n_gram * embedding_dim, hidden_dim)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.2)\n        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, x):\n        x = self.embedding(x).view(x.shape[0], -1)\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n\nclass RNNLanguageModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super(RNNLanguageModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        out, _ = self.rnn(x)\n        out = self.fc(out[:, -1, :])  # Take last output in the sequence\n        return out\n\n\nclass LSTMLanguageModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super(LSTMLanguageModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        out, _ = self.lstm(x)\n        out = self.fc(out[:, -1, :])  # Take last output in the sequence\n        return out\n\n\nclass TextDataset(Dataset):\n    def __init__(self, ngrams, vocab):\n        self.ngrams = ngrams\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.ngrams)\n\n    def __getitem__(self, idx):\n        context, target = self.ngrams[idx]\n        context_tensor = torch.tensor([self.vocab.get(word, 0) for word in context], dtype=torch.long)\n        target_tensor = torch.tensor(self.vocab.get(target, 0), dtype=torch.long)\n        return context_tensor, target_tensor\n\n\ndef generate_ngrams(tokenized_text, n):\n    ngrams = []\n    for sentence in tokenized_text:\n        if len(sentence) < n:\n            continue\n        for i in range(len(sentence) - n):\n            context = sentence[i:i + n]\n            target = sentence[i + n]\n            ngrams.append((context, target))\n    return ngrams\n\n\nclass Training:\n    def __init__(self, model, dataloader, vocab, inv_vocab, epochs=10, lr=0.001, save_path=\"/kaggle/working/\"):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(self.device)\n        self.model = model.to(self.device)\n        self.dataloader = dataloader\n        self.vocab = vocab\n        self.inv_vocab = inv_vocab\n        self.epochs = epochs\n        self.lr = lr\n        self.save_path = save_path\n\n    def train_model(self):\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n\n        for epoch in range(self.epochs):\n            total_loss = 0\n            for context, target in self.dataloader:\n                context, target = context.to(self.device), target.to(self.device)\n\n                optimizer.zero_grad()\n                output = self.model(context)\n                loss = criterion(output, target)\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n            print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(self.dataloader)}\")\n\n        os.makedirs(os.path.dirname(self.save_path), exist_ok=True)\n        torch.save(self.model.state_dict(), os.path.join(self.save_path,'model.pt'))\n        print(f\"Full model saved at {self.save_path}\")\n\n\ndef compute_perplexity2(model, dataloader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    total_log_prob = 0\n    total_words = 0\n\n    with torch.no_grad():\n        for context, target in dataloader:\n            context, target = context.to(device), target.to(device)\n            output = model(context)\n            log_probs = torch.log_softmax(output, dim=1)\n            batch_log_prob = log_probs[torch.arange(target.shape[0]), target]\n            total_log_prob += batch_log_prob.sum().item()\n            total_words += target.shape[0]\n\n    return torch.exp(torch.tensor(-total_log_prob / total_words))\n\n\n\ndef create_vocab(tokenized_text, path):\n    vocab = {word: idx for idx, word in enumerate(set(sum(tokenized_text, [])))}\n    inv_vocab = {idx: word for word, idx in vocab.items()}\n\n    os.makedirs(path, exist_ok=True)\n\n    vocab_path = os.path.join(path, \"vocab.pkl\")\n    inv_vocab_path = os.path.join(path, \"inv_vocab.pkl\")\n\n    with open(vocab_path, \"wb\") as f:\n        pickle.dump(vocab, f)\n\n    with open(inv_vocab_path, \"wb\") as f:\n        pickle.dump(inv_vocab, f)\n\n    print(f\"Vocabulary saved at {vocab_path} and {inv_vocab_path}\")\n    return vocab, inv_vocab\n\n\ndef split_dataset(ngram_data, test_size=1000):\n    test_set = random.sample(ngram_data, test_size)\n    train_set = [pair for pair in ngram_data if pair not in test_set]\n    return train_set, test_set\n\n\ndef predict_next_word(model, vocab, inv_vocab, n_gram, sentence, k):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    \n    if len(sentence) < n_gram:\n        print(\"Sentence too short!\")\n        return\n\n    context = sentence[-n_gram:]\n    context_tensor = torch.tensor([vocab.get(word, 0) for word in context], dtype=torch.long).unsqueeze(0).to(device)\n    output = model(context_tensor)\n    probs = torch.softmax(output, dim=1).squeeze()\n    top_k = torch.topk(probs, k)\n\n    for idx, prob in zip(top_k.indices, top_k.values):\n        print(f\"{inv_vocab[idx.item()]} {prob.item():.4f}\")\n\n\n# def main():\n# if __name__ == \"__main__\":\n#     main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T15:27:05.259475Z","iopub.execute_input":"2025-02-08T15:27:05.259877Z","iopub.status.idle":"2025-02-08T15:27:08.315420Z","shell.execute_reply.started":"2025-02-08T15:27:05.259852Z","shell.execute_reply":"2025-02-08T15:27:08.314686Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn.functional as F\n\ndef compute_and_save_perplexities(model, train_loader, test_loader, vocab, inv_vocab, model_name, n_gram, corpus_name):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()  # Set model to evaluation mode\n    \n    def compute_perplexity(loader, filename):\n        results_dir = \"/kaggle/working/results\"\n        os.makedirs(results_dir, exist_ok=True)\n        path = os.path.join(results_dir, filename)\n        print(filename)\n\n        total_log_prob = 0\n        total_words = 0\n        perplexity_lines = []\n\n        with torch.no_grad():\n            for context, target in loader:\n                context, target = context.to(device), target.to(device)\n                output = model(context)\n                log_probs = F.log_softmax(output, dim=1)\n                batch_log_prob = log_probs[torch.arange(target.shape[0]), target]\n                \n                total_log_prob += batch_log_prob.sum().item()\n                total_words += target.shape[0]\n\n                # Convert context indices back to words\n                context_words = [\" \".join(inv_vocab[idx.item()] for idx in sent) for sent in context]\n                target_words = [inv_vocab[target[i].item()] for i in range(target.shape[0])]\n\n                for ctx, tgt, ppl in zip(context_words, target_words, batch_log_prob):\n                    perplexity_lines.append(f\"{ctx} -> {tgt}\\t{torch.exp(-ppl).item():.4f}\\n\")\n\n        # Compute the average perplexity\n        avg_perplexity = torch.exp(torch.tensor(-total_log_prob / total_words)).item()\n\n        # Write to file with average perplexity at the top\n        with open(path, \"w\") as f:\n            f.write(f\"Average Perplexity: {avg_perplexity:.4f}\\n\")\n            f.writelines(perplexity_lines)\n\n        print(f\"Saved perplexity scores with sentences in: {path}\")\n\n    # Create filenames\n    train_filename = f\"{model_name}_train_{n_gram}_{corpus_name}.txt\"\n    test_filename = f\"{model_name}_test_{n_gram}_{corpus_name}.txt\"\n\n    # Compute perplexities for train and test\n    compute_perplexity(train_loader, train_filename)\n    compute_perplexity(test_loader, test_filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T15:27:08.316832Z","iopub.execute_input":"2025-02-08T15:27:08.317235Z","iopub.status.idle":"2025-02-08T15:27:08.324722Z","shell.execute_reply.started":"2025-02-08T15:27:08.317212Z","shell.execute_reply":"2025-02-08T15:27:08.324029Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# if len(sys.argv) < 4:\n#     print(\"Usage: python3 generator.py <lm_type> <corpus_path> <k>\")\n#     sys.exit(1)\n\n# lm_type = '-f'\n# corpus_path = '/kaggle/input/external/Pride and Prejudice - Jane Austen.txt'\n# corpus_path = '/kaggle/input/external/Ulysses - James Joyce.txt'\nimport os\n\nk = 3\n\nfor lm_type in ['-r', '-l', '-f']:\n    for corpus_path in ['/kaggle/input/external/Pride and Prejudice - Jane Austen.txt', '/kaggle/input/external/Ulysses - James Joyce.txt']:\n        for n_gram in [3, 5]:\n\n            with open(corpus_path, 'r', encoding='utf-8') as f:\n                text = f.read()\n\n            tokenized_text = custom_nlp_tokenizer(text)\n            corpus_name = os.path.basename(corpus_path).replace(\" \", \"_\").replace(\".txt\", \"\")\n\n            vocab_path = f\"/kaggle/working/{lm_type[1]}nn_{corpus_name}_n_{n_gram}\"\n            os.makedirs(os.path.dirname(vocab_path), exist_ok=True)\n            vocab, inv_vocab = create_vocab(tokenized_text,path=vocab_path)\n\n            ngram_data = generate_ngrams(tokenized_text, n_gram)  # Correct n_gram usage\n            train_set, test_set = split_dataset(ngram_data)\n\n            train_dataset = TextDataset(train_set, vocab)\n            test_dataset = TextDataset(test_set, vocab)\n            train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n            test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n\n            if lm_type == '-f':\n                model = FFNNLanguageModel(len(vocab), embedding_dim=100, hidden_dim=200, n_gram=n_gram)\n            elif lm_type == '-r':\n                model = RNNLanguageModel(len(vocab), embedding_dim=100, hidden_dim=200)\n            elif lm_type == '-l':\n                model = LSTMLanguageModel(len(vocab), embedding_dim=100, hidden_dim=200)\n            else:\n                print(\"Invalid model type! Use '-f' for FFNN, '-r' for RNN, '-l' for LSTM\")\n                sys.exit(1)\n\n            save_path = f\"/kaggle/working/{lm_type[1]}nn_{corpus_name}_n_{n_gram}\"\n\n            epochs = 5 if corpus_path=='/kaggle/input/external/Pride and Prejudice - Jane Austen.txt' else 15\n            Trainer = Training(model, train_loader, vocab, inv_vocab, epochs=epochs, lr=0.001, save_path=save_path)\n            Trainer.train_model()\n\n\n            perplexity = compute_perplexity2(model, train_loader)\n            print(f\"Train Perplexity for {lm_type} on {corpus_name} (n={n_gram}): {perplexity}\")\n            \n            perplexity = compute_perplexity2(model, test_loader)\n            print(f\"Test Perplexity for {lm_type} on {corpus_name} (n={n_gram}): {perplexity}\")\n\n            # print('Printing corpus_name',corpus_name)\n\n            compute_and_save_perplexities(model=model, train_loader=train_loader, test_loader=test_loader, vocab=vocab, inv_vocab=inv_vocab, model_name=lm_type[1] + \"nn\", n_gram=n_gram, corpus_name=corpus_name)\n            # print(f\"Avg Train Perplexity: {avg_train_ppl:.4f}, Avg Test Perplexity: {avg_test_ppl:.4f}\")\n\n# while True:\n#     sentence = input(\"Input sentence: \").strip().split()\n#     predict_next_word(model, vocab, inv_vocab, n_gram, sentence, k)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T15:27:08.325522Z","iopub.execute_input":"2025-02-08T15:27:08.325788Z","iopub.status.idle":"2025-02-08T15:53:24.025913Z","shell.execute_reply.started":"2025-02-08T15:27:08.325757Z","shell.execute_reply":"2025-02-08T15:53:24.025018Z"}},"outputs":[{"name":"stdout","text":"Vocabulary saved at /kaggle/working/rnn_Pride_and_Prejudice_-_Jane_Austen_n_3/vocab.pkl and /kaggle/working/rnn_Pride_and_Prejudice_-_Jane_Austen_n_3/inv_vocab.pkl\ncuda\nEpoch 1, Loss: 5.550588242790916\nEpoch 2, Loss: 4.744100943478671\nEpoch 3, Loss: 4.411708566925743\nEpoch 4, Loss: 4.152533916126598\nEpoch 5, Loss: 3.9281965780258177\nFull model saved at /kaggle/working/rnn_Pride_and_Prejudice_-_Jane_Austen_n_3\nTrain Perplexity for -r on Pride_and_Prejudice_-_Jane_Austen (n=3): 37.40426254272461\nTest Perplexity for -r on Pride_and_Prejudice_-_Jane_Austen (n=3): 143.08645629882812\nrnn_train_3_Pride_and_Prejudice_-_Jane_Austen.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/rnn_train_3_Pride_and_Prejudice_-_Jane_Austen.txt\nrnn_test_3_Pride_and_Prejudice_-_Jane_Austen.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/rnn_test_3_Pride_and_Prejudice_-_Jane_Austen.txt\nVocabulary saved at /kaggle/working/rnn_Pride_and_Prejudice_-_Jane_Austen_n_5/vocab.pkl and /kaggle/working/rnn_Pride_and_Prejudice_-_Jane_Austen_n_5/inv_vocab.pkl\ncuda\nEpoch 1, Loss: 5.570626295602369\nEpoch 2, Loss: 4.751729441254507\nEpoch 3, Loss: 4.420949338453089\nEpoch 4, Loss: 4.165909236598863\nEpoch 5, Loss: 3.943535414137859\nFull model saved at /kaggle/working/rnn_Pride_and_Prejudice_-_Jane_Austen_n_5\nTrain Perplexity for -r on Pride_and_Prejudice_-_Jane_Austen (n=5): 37.981422424316406\nTest Perplexity for -r on Pride_and_Prejudice_-_Jane_Austen (n=5): 118.11428833007812\nrnn_train_5_Pride_and_Prejudice_-_Jane_Austen.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/rnn_train_5_Pride_and_Prejudice_-_Jane_Austen.txt\nrnn_test_5_Pride_and_Prejudice_-_Jane_Austen.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/rnn_test_5_Pride_and_Prejudice_-_Jane_Austen.txt\nVocabulary saved at /kaggle/working/rnn_Ulysses_-_James_Joyce_n_3/vocab.pkl and /kaggle/working/rnn_Ulysses_-_James_Joyce_n_3/inv_vocab.pkl\ncuda\nEpoch 1, Loss: 6.262214052210066\nEpoch 2, Loss: 5.441986031182986\nEpoch 3, Loss: 4.935760196266629\nEpoch 4, Loss: 4.473353896474107\nEpoch 5, Loss: 4.048878313735559\nEpoch 6, Loss: 3.6800465579739625\nEpoch 7, Loss: 3.373175713028981\nEpoch 8, Loss: 3.119544062947496\nEpoch 9, Loss: 2.9059987724131986\nEpoch 10, Loss: 2.722434759343097\nEpoch 11, Loss: 2.5616377315894914\nEpoch 12, Loss: 2.4195302790839994\nEpoch 13, Loss: 2.292346981254103\nEpoch 14, Loss: 2.177578789107438\nEpoch 15, Loss: 2.0757505096609385\nFull model saved at /kaggle/working/rnn_Ulysses_-_James_Joyce_n_3\nTrain Perplexity for -r on Ulysses_-_James_Joyce (n=3): 6.1930341720581055\nTest Perplexity for -r on Ulysses_-_James_Joyce (n=3): 1952.6290283203125\nrnn_train_3_Ulysses_-_James_Joyce.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/rnn_train_3_Ulysses_-_James_Joyce.txt\nrnn_test_3_Ulysses_-_James_Joyce.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/rnn_test_3_Ulysses_-_James_Joyce.txt\nVocabulary saved at /kaggle/working/rnn_Ulysses_-_James_Joyce_n_5/vocab.pkl and /kaggle/working/rnn_Ulysses_-_James_Joyce_n_5/inv_vocab.pkl\ncuda\nEpoch 1, Loss: 6.3059690769489585\nEpoch 2, Loss: 5.485397704012759\nEpoch 3, Loss: 4.985712752566562\nEpoch 4, Loss: 4.522147068628916\nEpoch 5, Loss: 4.090036789337556\nEpoch 6, Loss: 3.7011962779410728\nEpoch 7, Loss: 3.3651615045927428\nEpoch 8, Loss: 3.0747262627274186\nEpoch 9, Loss: 2.8261976125123383\nEpoch 10, Loss: 2.6073783012004466\nEpoch 11, Loss: 2.4141886706824773\nEpoch 12, Loss: 2.2404492548158816\nEpoch 13, Loss: 2.0844747600135385\nEpoch 14, Loss: 1.9434922075605727\nEpoch 15, Loss: 1.8154212952376128\nFull model saved at /kaggle/working/rnn_Ulysses_-_James_Joyce_n_5\nTrain Perplexity for -r on Ulysses_-_James_Joyce (n=5): 4.824101448059082\nTest Perplexity for -r on Ulysses_-_James_Joyce (n=5): 1607.3035888671875\nrnn_train_5_Ulysses_-_James_Joyce.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/rnn_train_5_Ulysses_-_James_Joyce.txt\nrnn_test_5_Ulysses_-_James_Joyce.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/rnn_test_5_Ulysses_-_James_Joyce.txt\nVocabulary saved at /kaggle/working/lnn_Pride_and_Prejudice_-_Jane_Austen_n_3/vocab.pkl and /kaggle/working/lnn_Pride_and_Prejudice_-_Jane_Austen_n_3/inv_vocab.pkl\ncuda\nEpoch 1, Loss: 5.685569429137962\nEpoch 2, Loss: 4.822976641992476\nEpoch 3, Loss: 4.496723225242214\nEpoch 4, Loss: 4.2436242986287915\nEpoch 5, Loss: 4.0204986980735935\nFull model saved at /kaggle/working/lnn_Pride_and_Prejudice_-_Jane_Austen_n_3\nTrain Perplexity for -l on Pride_and_Prejudice_-_Jane_Austen (n=3): 42.58351516723633\nTest Perplexity for -l on Pride_and_Prejudice_-_Jane_Austen (n=3): 132.23121643066406\nlnn_train_3_Pride_and_Prejudice_-_Jane_Austen.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/lnn_train_3_Pride_and_Prejudice_-_Jane_Austen.txt\nlnn_test_3_Pride_and_Prejudice_-_Jane_Austen.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/lnn_test_3_Pride_and_Prejudice_-_Jane_Austen.txt\nVocabulary saved at /kaggle/working/lnn_Pride_and_Prejudice_-_Jane_Austen_n_5/vocab.pkl and /kaggle/working/lnn_Pride_and_Prejudice_-_Jane_Austen_n_5/inv_vocab.pkl\ncuda\nEpoch 1, Loss: 5.6733634490740625\nEpoch 2, Loss: 4.859463369422279\nEpoch 3, Loss: 4.5373506055990225\nEpoch 4, Loss: 4.296158232707751\nEpoch 5, Loss: 4.081443864366283\nFull model saved at /kaggle/working/lnn_Pride_and_Prejudice_-_Jane_Austen_n_5\nTrain Perplexity for -l on Pride_and_Prejudice_-_Jane_Austen (n=5): 46.308475494384766\nTest Perplexity for -l on Pride_and_Prejudice_-_Jane_Austen (n=5): 92.56452178955078\nlnn_train_5_Pride_and_Prejudice_-_Jane_Austen.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/lnn_train_5_Pride_and_Prejudice_-_Jane_Austen.txt\nlnn_test_5_Pride_and_Prejudice_-_Jane_Austen.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/lnn_test_5_Pride_and_Prejudice_-_Jane_Austen.txt\nVocabulary saved at /kaggle/working/lnn_Ulysses_-_James_Joyce_n_3/vocab.pkl and /kaggle/working/lnn_Ulysses_-_James_Joyce_n_3/inv_vocab.pkl\ncuda\nEpoch 1, Loss: 6.327309857014896\nEpoch 2, Loss: 5.493231139620956\nEpoch 3, Loss: 5.028238685358138\nEpoch 4, Loss: 4.586500862017781\nEpoch 5, Loss: 4.185212457869329\nEpoch 6, Loss: 3.823566117456981\nEpoch 7, Loss: 3.5100026063773098\nEpoch 8, Loss: 3.2315245260592222\nEpoch 9, Loss: 2.986538214444303\nEpoch 10, Loss: 2.7688832619563253\nEpoch 11, Loss: 2.5742441620550998\nEpoch 12, Loss: 2.400561809337058\nEpoch 13, Loss: 2.24338990084979\nEpoch 14, Loss: 2.101033404266753\nEpoch 15, Loss: 1.9737182653882877\nFull model saved at /kaggle/working/lnn_Ulysses_-_James_Joyce_n_3\nTrain Perplexity for -l on Ulysses_-_James_Joyce (n=3): 5.493143081665039\nTest Perplexity for -l on Ulysses_-_James_Joyce (n=3): 1684.0614013671875\nlnn_train_3_Ulysses_-_James_Joyce.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/lnn_train_3_Ulysses_-_James_Joyce.txt\nlnn_test_3_Ulysses_-_James_Joyce.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/lnn_test_3_Ulysses_-_James_Joyce.txt\nVocabulary saved at /kaggle/working/lnn_Ulysses_-_James_Joyce_n_5/vocab.pkl and /kaggle/working/lnn_Ulysses_-_James_Joyce_n_5/inv_vocab.pkl\ncuda\nEpoch 1, Loss: 6.378905473409353\nEpoch 2, Loss: 5.566117308638595\nEpoch 3, Loss: 5.124969411779333\nEpoch 4, Loss: 4.696880917410712\nEpoch 5, Loss: 4.275983744555408\nEpoch 6, Loss: 3.8776139213038876\nEpoch 7, Loss: 3.516028063194649\nEpoch 8, Loss: 3.1932250460585556\nEpoch 9, Loss: 2.902830400505104\nEpoch 10, Loss: 2.6454084017374613\nEpoch 11, Loss: 2.414059693032915\nEpoch 12, Loss: 2.206920994771971\nEpoch 13, Loss: 2.0186598595675527\nEpoch 14, Loss: 1.8486147726381625\nEpoch 15, Loss: 1.694015887167838\nFull model saved at /kaggle/working/lnn_Ulysses_-_James_Joyce_n_5\nTrain Perplexity for -l on Ulysses_-_James_Joyce (n=5): 4.170177459716797\nTest Perplexity for -l on Ulysses_-_James_Joyce (n=5): 1647.75390625\nlnn_train_5_Ulysses_-_James_Joyce.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/lnn_train_5_Ulysses_-_James_Joyce.txt\nlnn_test_5_Ulysses_-_James_Joyce.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/lnn_test_5_Ulysses_-_James_Joyce.txt\nVocabulary saved at /kaggle/working/fnn_Pride_and_Prejudice_-_Jane_Austen_n_3/vocab.pkl and /kaggle/working/fnn_Pride_and_Prejudice_-_Jane_Austen_n_3/inv_vocab.pkl\ncuda\nEpoch 1, Loss: 5.625541081862016\nEpoch 2, Loss: 4.806762591275302\nEpoch 3, Loss: 4.4594892328435725\nEpoch 4, Loss: 4.164822290160439\nEpoch 5, Loss: 3.8945250593532217\nFull model saved at /kaggle/working/fnn_Pride_and_Prejudice_-_Jane_Austen_n_3\nTrain Perplexity for -f on Pride_and_Prejudice_-_Jane_Austen (n=3): 35.81056594848633\nTest Perplexity for -f on Pride_and_Prejudice_-_Jane_Austen (n=3): 176.61001586914062\nfnn_train_3_Pride_and_Prejudice_-_Jane_Austen.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/fnn_train_3_Pride_and_Prejudice_-_Jane_Austen.txt\nfnn_test_3_Pride_and_Prejudice_-_Jane_Austen.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/fnn_test_3_Pride_and_Prejudice_-_Jane_Austen.txt\nVocabulary saved at /kaggle/working/fnn_Pride_and_Prejudice_-_Jane_Austen_n_5/vocab.pkl and /kaggle/working/fnn_Pride_and_Prejudice_-_Jane_Austen_n_5/inv_vocab.pkl\ncuda\nEpoch 1, Loss: 5.650716575238073\nEpoch 2, Loss: 4.821541641069495\nEpoch 3, Loss: 4.4452677776691\nEpoch 4, Loss: 4.120933515281074\nEpoch 5, Loss: 3.799384678776556\nFull model saved at /kaggle/working/fnn_Pride_and_Prejudice_-_Jane_Austen_n_5\nTrain Perplexity for -f on Pride_and_Prejudice_-_Jane_Austen (n=5): 30.270702362060547\nTest Perplexity for -f on Pride_and_Prejudice_-_Jane_Austen (n=5): 252.45362854003906\nfnn_train_5_Pride_and_Prejudice_-_Jane_Austen.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/fnn_train_5_Pride_and_Prejudice_-_Jane_Austen.txt\nfnn_test_5_Pride_and_Prejudice_-_Jane_Austen.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/fnn_test_5_Pride_and_Prejudice_-_Jane_Austen.txt\nVocabulary saved at /kaggle/working/fnn_Ulysses_-_James_Joyce_n_3/vocab.pkl and /kaggle/working/fnn_Ulysses_-_James_Joyce_n_3/inv_vocab.pkl\ncuda\nEpoch 1, Loss: 6.341855949239528\nEpoch 2, Loss: 5.570658008494275\nEpoch 3, Loss: 5.179389284417985\nEpoch 4, Loss: 4.803917789865047\nEpoch 5, Loss: 4.391596862508895\nEpoch 6, Loss: 3.9301082444698254\nEpoch 7, Loss: 3.530954438067497\nEpoch 8, Loss: 3.2577124496216467\nEpoch 9, Loss: 3.075774780435765\nEpoch 10, Loss: 2.9394958632043067\nEpoch 11, Loss: 2.835217500240245\nEpoch 12, Loss: 2.7456595437070157\nEpoch 13, Loss: 2.6681100340092434\nEpoch 14, Loss: 2.595626382016121\nEpoch 15, Loss: 2.531673944351521\nFull model saved at /kaggle/working/fnn_Ulysses_-_James_Joyce_n_3\nTrain Perplexity for -f on Ulysses_-_James_Joyce (n=3): 10.554923057556152\nTest Perplexity for -f on Ulysses_-_James_Joyce (n=3): 62173.33984375\nfnn_train_3_Ulysses_-_James_Joyce.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/fnn_train_3_Ulysses_-_James_Joyce.txt\nfnn_test_3_Ulysses_-_James_Joyce.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/fnn_test_3_Ulysses_-_James_Joyce.txt\nVocabulary saved at /kaggle/working/fnn_Ulysses_-_James_Joyce_n_5/vocab.pkl and /kaggle/working/fnn_Ulysses_-_James_Joyce_n_5/inv_vocab.pkl\ncuda\nEpoch 1, Loss: 6.427517684253009\nEpoch 2, Loss: 5.657800957963273\nEpoch 3, Loss: 5.245587015295172\nEpoch 4, Loss: 4.848178333706326\nEpoch 5, Loss: 4.403401759293702\nEpoch 6, Loss: 3.883217193700888\nEpoch 7, Loss: 3.3984804229812697\nEpoch 8, Loss: 3.0613178063679984\nEpoch 9, Loss: 2.8422918045246326\nEpoch 10, Loss: 2.685803901922476\nEpoch 11, Loss: 2.5637199463429035\nEpoch 12, Loss: 2.4635007429170654\nEpoch 13, Loss: 2.374604704740408\nEpoch 14, Loss: 2.293898853692445\nEpoch 15, Loss: 2.221402543801087\nFull model saved at /kaggle/working/fnn_Ulysses_-_James_Joyce_n_5\nTrain Perplexity for -f on Ulysses_-_James_Joyce (n=5): 7.647922992706299\nTest Perplexity for -f on Ulysses_-_James_Joyce (n=5): 354816.5\nfnn_train_5_Ulysses_-_James_Joyce.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/fnn_train_5_Ulysses_-_James_Joyce.txt\nfnn_test_5_Ulysses_-_James_Joyce.txt\nSaved perplexity scores with sentences in: /kaggle/working/results/fnn_test_5_Ulysses_-_James_Joyce.txt\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!zip -r everything.zip /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T15:53:24.026934Z","iopub.execute_input":"2025-02-08T15:53:24.027417Z","iopub.status.idle":"2025-02-08T15:53:47.325596Z","shell.execute_reply.started":"2025-02-08T15:53:24.027389Z","shell.execute_reply":"2025-02-08T15:53:47.324770Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/lnn_Ulysses_-_James_Joyce_n_5/ (stored 0%)\n  adding: kaggle/working/lnn_Ulysses_-_James_Joyce_n_5/model.pt (deflated 7%)\n  adding: kaggle/working/lnn_Ulysses_-_James_Joyce_n_5/inv_vocab.pkl (deflated 42%)\n  adding: kaggle/working/lnn_Ulysses_-_James_Joyce_n_5/vocab.pkl (deflated 42%)\n  adding: kaggle/working/fnn_Pride_and_Prejudice_-_Jane_Austen_n_3/ (stored 0%)\n  adding: kaggle/working/fnn_Pride_and_Prejudice_-_Jane_Austen_n_3/model.pt (deflated 8%)\n  adding: kaggle/working/fnn_Pride_and_Prejudice_-_Jane_Austen_n_3/inv_vocab.pkl (deflated 45%)\n  adding: kaggle/working/fnn_Pride_and_Prejudice_-_Jane_Austen_n_3/vocab.pkl (deflated 45%)\n  adding: kaggle/working/rnn_Ulysses_-_James_Joyce_n_5/ (stored 0%)\n  adding: kaggle/working/rnn_Ulysses_-_James_Joyce_n_5/model.pt (deflated 8%)\n  adding: kaggle/working/rnn_Ulysses_-_James_Joyce_n_5/inv_vocab.pkl (deflated 42%)\n  adding: kaggle/working/rnn_Ulysses_-_James_Joyce_n_5/vocab.pkl (deflated 42%)\n  adding: kaggle/working/lnn_Pride_and_Prejudice_-_Jane_Austen_n_5/ (stored 0%)\n  adding: kaggle/working/lnn_Pride_and_Prejudice_-_Jane_Austen_n_5/model.pt (deflated 7%)\n  adding: kaggle/working/lnn_Pride_and_Prejudice_-_Jane_Austen_n_5/inv_vocab.pkl (deflated 45%)\n  adding: kaggle/working/lnn_Pride_and_Prejudice_-_Jane_Austen_n_5/vocab.pkl (deflated 45%)\n  adding: kaggle/working/rnn_Ulysses_-_James_Joyce_n_3/ (stored 0%)\n  adding: kaggle/working/rnn_Ulysses_-_James_Joyce_n_3/model.pt (deflated 8%)\n  adding: kaggle/working/rnn_Ulysses_-_James_Joyce_n_3/inv_vocab.pkl (deflated 42%)\n  adding: kaggle/working/rnn_Ulysses_-_James_Joyce_n_3/vocab.pkl (deflated 42%)\n  adding: kaggle/working/rnn_Pride_and_Prejudice_-_Jane_Austen_n_5/ (stored 0%)\n  adding: kaggle/working/rnn_Pride_and_Prejudice_-_Jane_Austen_n_5/model.pt (deflated 7%)\n  adding: kaggle/working/rnn_Pride_and_Prejudice_-_Jane_Austen_n_5/inv_vocab.pkl (deflated 45%)\n  adding: kaggle/working/rnn_Pride_and_Prejudice_-_Jane_Austen_n_5/vocab.pkl (deflated 45%)\n  adding: kaggle/working/fnn_Pride_and_Prejudice_-_Jane_Austen_n_5/ (stored 0%)\n  adding: kaggle/working/fnn_Pride_and_Prejudice_-_Jane_Austen_n_5/model.pt (deflated 8%)\n  adding: kaggle/working/fnn_Pride_and_Prejudice_-_Jane_Austen_n_5/inv_vocab.pkl (deflated 45%)\n  adding: kaggle/working/fnn_Pride_and_Prejudice_-_Jane_Austen_n_5/vocab.pkl (deflated 45%)\n  adding: kaggle/working/results/ (stored 0%)\n  adding: kaggle/working/results/rnn_train_3_Pride_and_Prejudice_-_Jane_Austen.txt (deflated 57%)\n  adding: kaggle/working/results/fnn_test_3_Ulysses_-_James_Joyce.txt (deflated 50%)\n  adding: kaggle/working/results/fnn_train_5_Pride_and_Prejudice_-_Jane_Austen.txt (deflated 58%)\n  adding: kaggle/working/results/fnn_test_5_Pride_and_Prejudice_-_Jane_Austen.txt (deflated 54%)\n  adding: kaggle/working/results/fnn_test_5_Ulysses_-_James_Joyce.txt (deflated 50%)\n  adding: kaggle/working/results/fnn_train_5_Ulysses_-_James_Joyce.txt (deflated 54%)\n  adding: kaggle/working/results/fnn_train_3_Ulysses_-_James_Joyce.txt (deflated 54%)\n  adding: kaggle/working/results/rnn_train_5_Ulysses_-_James_Joyce.txt (deflated 54%)\n  adding: kaggle/working/results/lnn_train_5_Ulysses_-_James_Joyce.txt (deflated 54%)\n  adding: kaggle/working/results/rnn_test_5_Ulysses_-_James_Joyce.txt (deflated 50%)\n  adding: kaggle/working/results/lnn_test_5_Pride_and_Prejudice_-_Jane_Austen.txt (deflated 54%)\n  adding: kaggle/working/results/rnn_train_5_Pride_and_Prejudice_-_Jane_Austen.txt (deflated 58%)\n  adding: kaggle/working/results/lnn_test_5_Ulysses_-_James_Joyce.txt (deflated 50%)\n  adding: kaggle/working/results/fnn_test_3_Pride_and_Prejudice_-_Jane_Austen.txt (deflated 52%)\n  adding: kaggle/working/results/lnn_train_5_Pride_and_Prejudice_-_Jane_Austen.txt (deflated 58%)\n  adding: kaggle/working/results/lnn_train_3_Ulysses_-_James_Joyce.txt (deflated 54%)\n  adding: kaggle/working/results/lnn_test_3_Pride_and_Prejudice_-_Jane_Austen.txt (deflated 52%)\n  adding: kaggle/working/results/fnn_train_3_Pride_and_Prejudice_-_Jane_Austen.txt (deflated 57%)\n  adding: kaggle/working/results/rnn_test_3_Pride_and_Prejudice_-_Jane_Austen.txt (deflated 52%)\n  adding: kaggle/working/results/lnn_train_3_Pride_and_Prejudice_-_Jane_Austen.txt (deflated 57%)\n  adding: kaggle/working/results/lnn_test_3_Ulysses_-_James_Joyce.txt (deflated 50%)\n  adding: kaggle/working/results/rnn_test_5_Pride_and_Prejudice_-_Jane_Austen.txt (deflated 54%)\n  adding: kaggle/working/results/rnn_train_3_Ulysses_-_James_Joyce.txt (deflated 54%)\n  adding: kaggle/working/results/rnn_test_3_Ulysses_-_James_Joyce.txt (deflated 49%)\n  adding: kaggle/working/fnn_Ulysses_-_James_Joyce_n_5/ (stored 0%)\n  adding: kaggle/working/fnn_Ulysses_-_James_Joyce_n_5/model.pt (deflated 9%)\n  adding: kaggle/working/fnn_Ulysses_-_James_Joyce_n_5/inv_vocab.pkl (deflated 42%)\n  adding: kaggle/working/fnn_Ulysses_-_James_Joyce_n_5/vocab.pkl (deflated 42%)\n  adding: kaggle/working/lnn_Ulysses_-_James_Joyce_n_3/ (stored 0%)\n  adding: kaggle/working/lnn_Ulysses_-_James_Joyce_n_3/model.pt (deflated 7%)\n  adding: kaggle/working/lnn_Ulysses_-_James_Joyce_n_3/inv_vocab.pkl (deflated 42%)\n  adding: kaggle/working/lnn_Ulysses_-_James_Joyce_n_3/vocab.pkl (deflated 42%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/lnn_Pride_and_Prejudice_-_Jane_Austen_n_3/ (stored 0%)\n  adding: kaggle/working/lnn_Pride_and_Prejudice_-_Jane_Austen_n_3/model.pt (deflated 7%)\n  adding: kaggle/working/lnn_Pride_and_Prejudice_-_Jane_Austen_n_3/inv_vocab.pkl (deflated 45%)\n  adding: kaggle/working/lnn_Pride_and_Prejudice_-_Jane_Austen_n_3/vocab.pkl (deflated 45%)\n  adding: kaggle/working/rnn_Pride_and_Prejudice_-_Jane_Austen_n_3/ (stored 0%)\n  adding: kaggle/working/rnn_Pride_and_Prejudice_-_Jane_Austen_n_3/model.pt (deflated 7%)\n  adding: kaggle/working/rnn_Pride_and_Prejudice_-_Jane_Austen_n_3/inv_vocab.pkl (deflated 45%)\n  adding: kaggle/working/rnn_Pride_and_Prejudice_-_Jane_Austen_n_3/vocab.pkl (deflated 45%)\n  adding: kaggle/working/fnn_Ulysses_-_James_Joyce_n_3/ (stored 0%)\n  adding: kaggle/working/fnn_Ulysses_-_James_Joyce_n_3/model.pt (deflated 9%)\n  adding: kaggle/working/fnn_Ulysses_-_James_Joyce_n_3/inv_vocab.pkl (deflated 42%)\n  adding: kaggle/working/fnn_Ulysses_-_James_Joyce_n_3/vocab.pkl (deflated 42%)\n","output_type":"stream"}],"execution_count":5}]}